# akaza-default-model

## What's this?

Akaza のデフォルトのモデルデータ及び辞書データの作成レポジトリです。

## How to build this?

Akaza を利用する上では、ユーザーは自分でビルドする必要はありません。
Github Releases からビルド済みファイルを取得してください。

自分でビルドしたい場合は以下のようにしてビルドできます。

    make

## Dependencies

* wikiextractor
* python3
* wget
* rust

## How it works?

1. 日本語版 wikipedia の jawiki-latest-pages-articles.xml.bz2 を取得
2. bunzip2 で伸長
3. wikiextractor で text/ 以下に展開する
4. tokenize 処理する
5. wfreq(単語の頻度ファイル)を作成する
6. vocab(高頻度で出ている単語ファイル)を作成する
7. unigram の単語頻度情報ファイルを作成する
8. bigram の単語頻度情報ファイルを作成する
9. コーパスの情報で補正する(Wikipedia/青空文庫をソースにしている関係上、偏りが強いため一般的に利用されるかな漢字変換エンジンとしてふさわしいように補正しています)
10. システム辞書を作成(Akaza のシステム辞書は、コーパスに含まれる複合語を利用するためにつけているものであって、これだけで十分に変換ができるように設計されているわけではありません。SKK-JISYO.L などと併用することを想定しています)

## データフォーマット

### corpus/*.txt (学習コーパス)

`漢字/よみ` の形式でスペース区切り。`;; `で始まる行はコメント。

```
渡せない/わたせない のね/のね
僕/ぼく の/の 主観/しゅかん では/では そう/そう です/です
```

各単語が `表層形/読み` のペアで、文全体がスペースで区切られています。
コーパスに書かれた単語はシステム辞書にも自動的に登録されます。

### mecab-user-dict.csv (MeCab ユーザー辞書)

Vibrato (MeCab互換) のユーザー辞書 CSV フォーマット。トーカナイズに失敗する単語を追加します。

```
令和,1288,1288,5904,名詞,固有名詞,一般,*,*,*,令和,レイワ,レイワ
```

フィールド: `表層形,左文脈ID,右文脈ID,コスト,品詞,品詞細分類1,品詞細分類2,品詞細分類3,活用型,活用形,原形,読み,発音`

### dict/SKK-JISYO.akaza (ベースシステム辞書)

SKK 辞書形式。SKK-JISYO.L に含まれない語彙を登録するためのもの。

```
きめつのやいば /鬼滅の刃/
かなにゅうりょく /かな入力/仮名入力/
```

フォーマット: `よみ /候補1/候補2/.../`（複数候補はスラッシュ区切り）

### anthy-corpus/*.txt (評価用コーパス)

anthy-unicode 由来の評価用データ。各行は `|`区切りの読み列と漢字列のペア。

```
|さとう|」|です| |佐藤|」|です|
|あいての|ほうが|さきに|はんだんを|くだしていた| |相手の|方が|先に|判断を|下していた|
```

前半が読み(ひらがな)、スペースを挟んで後半が期待される変換結果。各文節は `|` で区切り。
corpus.4.txt は誤変換を収めたものなので評価には使いません。

### bigram.model, unigram.model (生成物)

marisa-trie 形式のデータです。1gram, 2gram のデータが素直に格納されています。

フォーマットは、2gram の場合は以下のようになっています。

```
愛/あい\tは/は => -0.525252
```

浮動小数点数がスコアです。このスコアは、n-gram の確率の -log10 です。

### data/SKK-JISYO.akaza (生成物)

ビルドで生成されるシステム辞書。dict/SKK-JISYO.akaza、コーパスの語彙、vibrato の vocab から、SKK-JISYO.L に含まれる語彙を除いたものが登録されます。フォーマットは dict/SKK-JISYO.akaza と同一です。

## 調整方法

### 誤変換がおきる理由

大前提として、日本語という言語の特性上、かな漢字変換の変換ミスを完全に０にすることはできません。

また、OSS でやっている以上、大規模な自然言語データを利用することは難しいです。大規模な自然言語データを処理するには個人のPCでは難しいので、課金してクラウドでコンピューティングリソースを利用したりしないとまぁ難しいでしょう。また、そもそも自然言語資源は著作権処理の関係上、かなりの金額を支払わないと利用できません。。

Akaza は「OSS で継続的に開発可能なかな漢字変換エンジン」を標榜していることから、Wikipedia と青空文庫という、まとまったデータをダウンロード可能で、無料で使える Github Actions のリソースクオータの中で処理可能な分量のデータを利用するようにしています。

もちろん、BCCWJ などの均衡コーパスを利用すればより高精度のモデルを容易に実装可能なので、誰かモデルデータつくって公開してくれたら嬉しいです。

### 誤変換の対応

#### 変換候補がそもそも出てこない

変換候補がそもそも出てこない場合は、辞書を追加してください。

SKK-JISYO.L や SKK-JISYO.jinmei などの、SKK の公式の辞書を設定に追加して解決すればそれが良いです。

芸能人の名前などの固有名詞が出てこない場合は、[jawiki-kana-kanji-dict](https://github.com/tokuhirom/jawiki-kana-kanji-dict) を追加してください。

#### Wikipedia に出てこないような用例を追加したい

Akaza のデフォルトモデルは、Wikipedia/青空文庫に含まれる表現の統計情報を利用している関係上、それらに含まれない表現にはどうしても齢です。特に口語表現には弱い部分があります。

変換できない表現があった場合は、コーパスに追加することで対応できます。

corpus/should.txt に追加の PR を送ってください。
書き方は corpus/README.md に書いてあります。

#### Wikipedia に入っているはずなのにうまく変換できない。

複合語化が可能な場合は、複合語として辞書に登録してください。

vibrato がトーカナイズに失敗してるな、と思った場合は mecab-user-dict.csv に単語を追加する PR を送ってください。

### 現在、どうしようもない誤変換

Akaza は単語2グラムを採用しており、共起的な情報を現在 Akaza では対応していません。
つまり、隣合う2つの単語がいっしょに書かれる確率が高いかどうかだけを判断しているということです。

であるから、「夏/は/暑い」と「この/板/は/厚い」を両方ちゃんと変換できるように Akaza を教育することはできません。

この点は、共起情報を活用すれば実装できないこともないので、今後なんらかの方法で対応したいところです。


